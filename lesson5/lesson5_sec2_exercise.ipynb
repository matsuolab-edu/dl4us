{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson5 画像からキャプションを生成してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- Section 2 実装①\n",
    "    - 2.0 データの用意\n",
    "    - 2.1 モデル構築\n",
    "    - 2.2 モデルの学習\n",
    "    - 2.3 モデルによる予測\n",
    "    - 2.4 パラメータの保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5の解答\n",
    "問1: ①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 実装①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS-COCOのデータセットでキャプション生成を実際に行ってみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path, tokenizer = None):\n",
    "    whole_texts = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "        \n",
    "    if tokenizer == None :\n",
    "        tokenizer = Tokenizer(filters=\"\")\n",
    "        tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "x_train = np.load('./data/x_train.npy')\n",
    "x_valid = np.load('./data/x_valid.npy')\n",
    "\n",
    "# 読み込み＆Tokenizerによる数値化\n",
    "y_train, tokenizer_train = load_data('./data/y_train.txt')\n",
    "y_valid, _ = load_data('./data/y_valid.txt', tokenizer_train)\n",
    "\n",
    "vocab_size = len(tokenizer_train.word_index) + 1\n",
    "\n",
    "# パディング\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "y_valid = pad_sequences(y_valid, padding='post')\n",
    "\n",
    "caption_len = len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "今回は学習時間・難易度の観点からEncoderのパラメータについてはVGG16のものをそのまま流用し、改めてチューニングは行わないことにします。\n",
    "\n",
    "余力のある方は、パラメータを固定せずに全体を学習することを試みてもよいかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Flatten, Lambda\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "encoder_input = Input(shape=(224, 224, 3))\n",
    "encoder_input_normalized = Lambda(lambda x: x / 255.)(encoder_input) # [0, 255) -> [0, 1)\n",
    "encoder = VGG16(weights='imagenet', include_top=False, input_tensor=encoder_input_normalized)\n",
    "\n",
    "# パラメータを固定\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# CNNの出力\n",
    "u = Flatten()(encoder.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "emb_dim = 128\n",
    "hid_dim = 128\n",
    "\n",
    "# LSTMの初期状態\n",
    "decoded_states = [Dense(hid_dim)(u), Dense(hid_dim)(u)] # h_0、c_0 に対応\n",
    "\n",
    "# 層の定義\n",
    "decoder_input = Input(shape=(caption_len,))\n",
    "embedding = Embedding(vocab_size, emb_dim, mask_zero=True)\n",
    "lstm = LSTM(hid_dim, activation='tanh', return_sequences=True, return_state=True)\n",
    "dense = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "# 層の接続\n",
    "decoder_embedded = embedding(decoder_input)\n",
    "decoder_output, _, _ = lstm(decoder_embedded, initial_state=decoded_states) # 第2、3戻り値(最終ステップのh、c)は無視\n",
    "decoder_pred = dense(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_pred)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=64, epochs=15, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 モデルによる生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にキャプション生成をvalidationデータに対して行いたいと思いますが、基本的な考え方は翻訳モデルと大差なく、入力が画像に変わるだけです。\n",
    "\n",
    "なお、decoderでキャプションを生成する際は、最初の単語として`<s>`を入力するのでした。\n",
    "\n",
    "その後各ステップで最大の確率を示す単語を採用し、次ステップの入力としていきます。\n",
    "\n",
    "最終的に終端記号`</s>`が排出されるか、もしくは予めきめておいた`max_len`に到達するまでdecodeしていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリング用（生成用）のモデルを作成\n",
    "\n",
    "# 符号化器（学習時と同じ構成、学習したレイヤーを利用）\n",
    "encoder_model = Model(encoder_input, decoded_states)\n",
    "\n",
    "# 復号化器\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))] # decorder用lstmの初期状態指定用(h_t、c_t)\n",
    "\n",
    "decoder_input = Input(shape=(1,))\n",
    "decoder_embedded = embedding(decoder_input) # 学習済みEmbeddingレイヤーを利用\n",
    "decoder_output, *decoder_states = lstm(decoder_embedded, initial_state=decoder_states_inputs) # 学習済みLSTMレイヤーを利用\n",
    "decoder_pred = dense(decoder_output) # 学習済みDenseレイヤーを利用\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_pred] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_image, bos_eos, max_output_length = 100):\n",
    "    states = encoder_model.predict(input_image)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    output_seq= bos_eos[0]\n",
    "    \n",
    "    while True:\n",
    "        token_dist, *states = decoder_model.predict([target_seq] + states)\n",
    "        sampled_token_index = [np.argmax(token_dist[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "        \n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bos_eos = tokenizer_train.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "detokenizer_train = dict(map(reversed, tokenizer_train.word_index.items()))\n",
    "\n",
    "x_test = np.array(x_valid[:1])\n",
    "\n",
    "y_pred = decode_sequence(x_test, bos_eos)\n",
    "\n",
    "print(' '.join([detokenizer_train[i] for i in y_pred]))\n",
    "\n",
    "plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 パラメータの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, name):\n",
    "    data_dir = 'data'\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    result_dir = os.path.normpath(data_dir)\n",
    "    model.save_weights(os.path.join(result_dir, name + '_model.h5'))\n",
    "    \n",
    "def load_weight(model, name):\n",
    "    data_dir = 'data'\n",
    "    result_dir = os.path.normpath(data_dir)\n",
    "    weight_file = os.path.join(result_dir, name + '_model.h5')\n",
    "    model.load_weights(weight_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_model(model, 'image2text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "if os.path.exists('data/image2text_model.h5'):\n",
    "    model = load_weight(model, 'image2text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
